---
title: "01_cln_corpus_creation"
format: md
editor: visual
---

## load packages

```{r, message=FALSE}
library(tidyverse)
library(tidytext)
```

## load data

```{r}
fh <- list.files("../corpus_raw/", full.names = T)

t <- tibble(
  path = fh,
  text = sapply(path, read_file)
)
```

## cleaning

### tokenisation

```{r}
tokens <- t %>% 
  # separate conjunctions such as j'ai -> j ai 
  mutate(text = str_replace_all(text, "(\\w)'|â€™(\\w)", "\\1' \\2")) %>% 
  
  unnest_tokens(input = text, output = word, token = "words") %>% 
  filter(!str_detect(word, "\\d+")) 

head(tokens)
```

### mfw cleaning

```{r}
tokens %>% 
  filter(nchar(word) > 1) %>% 
  count(word, sort = T) %>% head(300)
```

```{r}
tokens_cln <- tokens %>% 
  filter(nchar(word) > 1) %>% 
  mutate(word = str_replace_all(word, "^fa$", "sa"),
         word = str_replace_all(word, "^fur$", "sur"),
         word = str_replace_all(word, "^eft$", "est"),
         word = str_replace_all(word, "^fe$", "se"),
         word = str_replace_all(word, "^fi$", "si"),
         word = str_replace_all(word, "^fes$", "ses"),
         word = str_replace_all(word, "^fon$", "son"),
         word = str_replace_all(word, "^fans$", "sans"),
         word = str_replace_all(word, "^fol$", "sol"),
         word = str_replace_all(word, "^foit$", "soit"),
         word = str_replace_all(word, "^fous$", "sous"),
         word = str_replace_all(word, "^fera$", "sera"),
         word = str_replace_all(word, "^feroit$", "seroit"),
         word = str_replace_all(word, "^mefmer$", "mesmer"),
         word = str_replace_all(word, "^ut$", "et"),
         
         # questionable replacement:
         word = str_replace_all(word, "^font$", "sont")
  ) %>% 
  filter(!str_detect(word, "Digitized|by|knOOQLe|CnOOQie|Galerie|universelle|galerie|Universelle|Google"))
```

```{r}
corpus_cln <- tokens_cln %>% 
  group_by(path) %>% 
  mutate(text = paste(word, collapse = " ")) %>% 
  ungroup() %>% 
  select(-word) %>% 
  distinct() %>% 
  mutate(path_new = str_replace(path, "corpus_raw/", "corpus_texts/"))

glimpse(corpus_cln)
```

## save corpus_texts/

```{r}
for (i in 1:nrow(corpus_cln)) {
  write_file(corpus_cln$text[i], file = corpus_cln$path_new[i])
}
```

## corpus_authors/

```{r}
glimpse(corpus_cln)
```

Extract author's names from the metadata & filter texts in question

```{r}
corpus_authors <- corpus_cln %>% 
  # create a column with author's name
  mutate(author = str_remove_all(path, "\\.\\./corpus_raw//"),
         author = str_extract(author, "^.*?_"),
         author = str_remove_all(author, "_")) %>% 
  # filter only ground truth authors
  filter(!author %in% c("G1785", "G1787")) 

length(unique(corpus_authors$author))
```

### large texts

Count number of tokens in each text & select authors which texts should be trimmed

```{r}
n_tokens_text <- corpus_authors %>% 
  unnest_tokens(input = text, output = word, token = "words") %>% 
  count(path, author, sort = T) 

n_tokens_text

large_texts <- n_tokens_text %>% 
  filter(n > 15000) %>%  # filter texts with more than 15,000 words
  pull(path)
  
large_texts
```

### reshuffling

### large texts sampling

Select smaller samples from large texts

```{r}
authors_large_texts_s <- corpus_authors %>% 
  filter(path %in% large_texts) %>% 
  unnest_tokens(input = text, output = word, token = "words") %>% 
  group_by(path) %>% 
  sample_n(size = 15000) 
```

### all texts samples

```{r}
# glimpse(corpus_authors)
# glimpse(authors_large_texts_s)

corpus_authors_merged <- corpus_authors %>% 
  filter(!path %in% large_texts) %>% 
  unnest_tokens(input = text, output = word, token = "words") %>% 
  rbind(authors_large_texts_s) %>% 
  group_by(author) %>% 
  do(sample_n(., size = nrow(.))) %>% 
  summarise(text = paste(!!sym("word"), collapse = " ")) %>% 
  mutate(path = paste0("../corpus_authors//", author, ".txt"))

glimpse(corpus_authors_merged)
```

Number of tokens by each author

```{r}
corpus_authors_merged %>% 
  unnest_tokens(input = text, output = word, token = "words") %>% 
  count(author, sort = T)
```

## save authors' reshuffled texts

```{r}
for (i in 1:nrow(corpus_authors_merged)) {
  write_file(file = corpus_authors_merged$path[i], corpus_authors_merged$text[i])
}
```
