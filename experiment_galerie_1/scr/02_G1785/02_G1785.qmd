---
title: "02_G1785"
format: md
editor: visual
---

# Authorship of the introductory texts in Galerie 1785

### load packages

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidytext)

library(stylo)
library(seetrees)

set.seed(1234)
```

## Quick book-based exploration

### write data

```{r, eval = FALSE}
fh <- list.files(path = "../../corpus_texts/", full.names = T)

corpus <- tibble(
  path = fh,
  text = sapply(path, read_file),
  id = str_remove_all(path, "\\.\\./\\.\\./corpus_texts//|\\.txt"),
  path_new = str_remove_all(path, "^\\.\\./\\.\\./")
) %>% 
  filter(!str_detect(id, "^G1787")) # remove texts from 1787 volumes

glimpse(corpus)
```

Write texts for easier stylo use

```{r, eval = FALSE}
for (i in 1:nrow(corpus)) {
  write_file(corpus$text[i], file = corpus$path_new[i])
}
```

### stylo - quick tests

#### 200 mfw

```{r}
test_00 <- stylo(
  gui = F,
  corpus.dir = "corpus_texts/", # given that texts are already created
  corpus.lang = "Other",
  analysed.features = "w",
  mfw.min = 200,
  mfw.max = 200,
  distance.measure = "wurzburg"
)
```

```{r}
view_tree(test_00, k = 2, label_size = 4)
```

#### bootstrap

```{r, message=FALSE, warning=FALSE}
bct <- stylo(
  gui = F,
  mfw.min = 100,
  mfw.max = 500,
  mfw.incr = 1,
  analysed.features = "w",
  ngram.size = 1,
  analysis.type = "BCT",
  consensus.strength = 0.5,
  distance.measure = "wurzburg",
  corpus.dir = "corpus_texts/",
  corpus.lang = "Other"
)
```

```{r}
rm(bct, test_00, corpus, fh)
```

# Authors' samples & sampling

## create data

```{r, eval = FALSE}
fh_authors <- list.files(path = "../../corpus_authors/", 
                 full.names = T)

fh_q <- list.files(path = "../../corpus_texts/",
                   pattern = "^G1785",
                   full.names = T)

fh <- c(fh_authors, fh_q)
rm(fh_authors, fh_q)

corpus <- tibble(
  path = fh,
  text = sapply(path, read_file)
) %>% 
  mutate(author = str_remove_all(path, "\\.\\./\\.\\./.*?//|\\.txt"),
         path_new = str_remove_all(path, "\\.\\./\\.\\./.*?//"),
         path_new = paste0("corpus_authors//", path_new)) 

glimpse(corpus)
unique(corpus$author)
unique(corpus$path_new)
```

### save

```{r, eval = FALSE}
for (i in 1:nrow(corpus)) {
  write_file(corpus$text[i], file = corpus$path_new[i])
}
```

## load data

```{r}
fh <- list.files(path = "corpus_authors/",
                 full.names = T)

corpus <- tibble(
  path = fh,
  text = sapply(path, read_file)
) %>% 
  mutate(author = str_remove_all(path, "corpus_authors//|\\.txt")) 

glimpse(corpus)
```

### corpus_size

```{r}
corpus %>% 
  unnest_tokens(input = text, output = word, token = "words") %>% 
  count(author, sort = T)
```

## Exploration

### mfw 200

```{r}
test_200mfw <- stylo(
  gui = F,
  mfw.min = 200,
  mfw.max = 200,
  analyzed.features = "w",
  distance.measure = "wurzburg",
  corpus.dir = "corpus_authors/",
  corpus.lang = "Other"
  )
```

### bootstrap, all

```{r, warning=FALSE, message=FALSE}
bct_all <- stylo(
  gui = F,
  mfw.min = 100,
  mfw.max = 500,
  mfw.incr = 1,
  analyzed.features = "w",
  ngram.size = 1,
  analysis.type = "BCT",
  distance.measure = "wurzburg",
  corpus.dir = "corpus_authors/",
  corpus.lang = "Other"
)
```

## Sampling

### function

```{r}
sample_independent <- function(corpus,
                               sample_size,
                               n_samples,
                               text_var = "word",
                               folder = "corpus_samples_dump/",
                               overwrite = T) {
  
  # 0 create/rewrite folder
  dir.create(folder)
  
  if(overwrite) {
    do.call(file.remove, list(
      list.files("corpus_samples_dump/", full.names = TRUE))
      )
  }
  
  # I tokenization & ceiling
  
  corpus_tokenized <- corpus %>% 
    unnest_tokens(input = text, output = word, token = "words") %>% 
    
    group_by(author) %>% 
    # reshuffle words inside each author
    do(sample_n(., size = nrow(.))) %>%
    
    mutate(sample = ceiling(row_number()/sample_size)) %>% 
    unite(sample_id, c(author, sample), remove = F) %>% 
    filter(sample != max(sample)) %>% 
    ungroup() %>% 
    select(sample_id, author, word, sample)
  
  # II set seeds
  seed_samples <- corpus_tokenized %>% 
      select(sample_id, author) %>% 
      distinct() %>% 
      group_by(author) %>% 
      sample_n(n_samples) %>% 
      pull(sample_id)
  
  ### III create table with 1 row = 1 sample
    true_samples <- corpus_tokenized %>% 
      filter(sample_id %in% seed_samples) %>% 
      group_by(sample_id) %>% 
      summarise(text = paste(!!sym(text_var), collapse = " "))
    
    
  ### IV write samples in the folder
    for (i in 1:nrow(true_samples)) {
      write_file(file = paste0(folder, true_samples$sample_id[i], ".txt"),
                 true_samples$text[i])
    }
  
}
```

### test samples

#### 100 mfw

```{r}
sample_independent(corpus = corpus,
                   sample_size = 1800,
                   n_samples = 2)
```

```{r, message=FALSE}
test_samples_0 <- stylo(gui=F,
            mfw.min=100,
            mfw.max=100,
            analyzed.features = "w",
            ngram.size = 1,
            distance.measure = "wurzburg",
            #number.of.samples=2,
            corpus.lang="Other",
            corpus.dir="corpus_samples_dump/")
```

```{r}
view_tree(test_samples_0, k = 2, label_size = 4)
```

```{r}
test_samples_0$features.actually.used
```

#### 200 mfw

```{r}
sample_independent(corpus = corpus,
                   sample_size = 1800,
                   n_samples = 2)
```

```{r, message=FALSE}
test_samples_1 <- stylo(gui=F,
            mfw.min=200,
            mfw.max=200,
            analyzed.features = "w",
            ngram.size = 1,
            distance.measure = "wurzburg",
            #number.of.samples=2,
            corpus.lang="Other",
            corpus.dir="corpus_samples_dump/")
```

```{r}
view_tree(test_samples_1, k = 2, label_size = 4)
```

```{r}
test_samples_1$features.actually.used
```

### bct samples

```{r, warning=FALSE, message=FALSE}
bct_samples <- stylo(
            gui=F,
            mfw.min=100,
            mfw.max=200,
            mfw.incr = 1,
            analyzed.features = "w",
            ngram.size = 1,
            analysis.type = "BCT",
            consensus.strength = 0.5,
            distance.measure = "wurzburg",
            corpus.dir="corpus_samples_dump/",
            corpus.lang="Other"
            )
```

## Larger samples

Remove the smallest text G1785_4

```{r}
corpus %>% 
  unnest_tokens(input = text, output = word, token = "words") %>% 
  count(author, sort = T)

corpus_l <- corpus %>% 
  filter(author != "G1785_4_Discours préliminaire")
```

### test samples

#### 3 samples, 2k words

```{r}
sample_independent(corpus = corpus_l,
                   sample_size = 3000,
                   n_samples = 2)
```

```{r}
test_samples_2 <- stylo(gui=F,
            mfw.min=200,
            mfw.max=200,
            analyzed.features = "w",
            ngram.size = 1,
            distance.measure = "wurzburg",
            #number.of.samples=2,
            corpus.lang="Other",
            corpus.dir="corpus_samples_dump/")
```

```{r}
view_tree(test_samples_2, k = 2, label_size = 4)
```

```{r}
test_samples_2$features.actually.used
```

# Remove words

Filter out topic-related words

```{r}
glimpse(corpus)
```

Select words based on the features above

```{r}
signal_words <- paste0(c("^langue", "^latin", "^fut", "liberté",
                  "^gaule", "^lettre", "^hui$",
                  "^gaulois", "^language", "^ainfi", "^ainsi",
                  "^louis$", "^mesmer$", "frédéric",
                  "^génie", 
                  "^loi","^ancien", "^roi$",
                  "^liberté$", "^france$", "^francois",
                  
                  "^je$", "^ma$", "^mon$", "^moi$",
                  "^vous$", "^vos$", "^votre$"
                  ),
                  collapse = "|"
                  )
```

Write 1 text = 1 row

```{r}
corpus_filtered <- corpus %>% 
  unnest_tokens(input = text, output = word, token = "words") %>% 
  mutate(word = str_replace_all(word, "^ufage$", "usage"),
         word = str_replace_all(word, "^fiècle", "siècle")) %>% 
  filter(!str_detect(word, signal_words))
  
```

### n tokens after filter

```{r}
nrow(corpus_filtered)

corpus_filtered %>% 
  count(author, sort = T)

corpus_filtered %>% 
  filter(word == "ufage")
```

```{r}
corpus_filtered <- corpus_filtered %>% 
  group_by(author) %>% 
  mutate(text = paste(word, collapse = " ")) %>% 
  ungroup() %>% 
  select(-word) %>% 
  distinct()

glimpse(corpus_filtered)
```

## sampling

```{r}
sample_independent(corpus = corpus_filtered,
                   sample_size = 1800,
                   n_samples = 2)
```

## tests

### 100 mfw

```{r, message=FALSE}
test_filtered_100 <- stylo(
  gui = F,
  mfw.min = 100,
  mfw.max = 100,
  analyzed.features = "w",
  distance.measure = "wurzburg",
  corpus.dir = "corpus_samples_dump/",
  corpus.lang = "Other"
)
```

```{r, message=FALSE}
test_filtered_200 <- stylo(
  gui = F,
  mfw.min = 200,
  mfw.max = 200,
  analyzed.features = "w",
  distance.measure = "wurzburg",
  corpus.dir = "corpus_samples_dump/",
  corpus.lang = "Other"
)
```

```{r}
view_tree(test_filtered_200, k = 2, label_size = 4)
```

```{r}
test_filtered_200$features.actually.used
```

```{r, warning=FALSE, message=FALSE}
test_filtered_bct <- stylo(
            gui=F,
            mfw.min=50,
            mfw.max=200,
            mfw.incr = 1,
            analyzed.features = "w",
            ngram.size = 1,
            analysis.type = "BCT",
            consensus.strength = 0.5,
            distance.measure = "wurzburg",
            corpus.dir="corpus_samples_dump/",
            corpus.lang="Other"
)
```

# Distribution of distances

Replication from navalny.R https://github.com/perechen/navalny_R/blob/main/stylometry_navalny.md

## functions

```{r}
### function to process distance table
process_distances <- function(stylo_res) {
  dt <- stylo_res %>% as.matrix()
d_long <- dt %>% 
  as_tibble() %>%
  mutate(source=colnames(dt)) %>% 
  pivot_longer(1:nrow(dt),names_to="target",values_to ="distance") %>% 
  mutate(source=str_remove(source, "_[0-9]*$"),
         target=str_remove(target, "_[0-9]*$")) %>% 
  filter(distance != 0)
return(d_long)

}

## function to plot distance distributions 
plot_distances <- function(df) {
  df %>% 
  ggplot(aes(distance,fill=mean)) + 
    geom_density(alpha=0.6) + 
    geom_vline(aes(xintercept=mean)) + 
    facet_grid(source ~ target,scales = "free_y") + 
    theme_bw() + 
    scale_fill_gradient2(high=lannister[1],
                         mid=lannister[3],
                         low=lannister[5],
                         midpoint=1.05)
  
}

```

## sampling

### function

```{r}
sample_independent_opt <- function(tokenized_df,
                                   n_samples,
                                   sample_size,
                                   text_var="word",
                                   folder="corpus_sampled_opt/",
                                   overwrite=T) {
  
    dir.create(folder)
    
    if(overwrite) {
    do.call(file.remove, list(list.files(folder, full.names = TRUE)))
    }
    
    shuff <- tokenized_df %>%
      group_by(author) %>% 
      sample_n(n_samples*sample_size) %>% 
      mutate(sample=sample(rep(1:n_samples, each=sample_size))) %>% 
      unite(sample_id,c(author,sample),remove = F) %>% 
      group_by(sample_id) %>%
      summarize(text=paste(!!sym(text_var), collapse=" "))
    
    for(i in 1:nrow(shuff)) {
    write_file(file=paste0(folder, shuff$sample_id[i],".txt"), shuff$text[i])
  }

}

```

```{r}
glimpse(corpus)

corpus %>% filter(str_detect(author, "G1785")) %>% pull(author) %>% unique

# filter only Preface & Discours prelim text in question
removed_texts <- paste0(c("G1785_1", "G1785_2", "G1785_3"), collapse = "|")

corpus_t <- corpus %>% 
  filter(!str_detect(author, removed_texts)) %>% 
  unnest_tokens(input = text, output = word, token = "words")

head(corpus_t)
```

## dist iterations

Settings

```{r}
iters <- 100
min_features <- 50
max_features <- 500

d_res_w <- vector(mode = "list", 
                length = iters)
```

```{r, message=FALSE, warning=FALSE}
for (i in 1:iters) {
  
  mfw <- sample(seq(min_features,max_features, by=10),1)
  
  sample_independent_opt(corpus_t,
                   sample_size=1800,
                   text_var = "word",
                   n_samples = 2)

  tokenised_texts <- load.corpus.and.parse(files = list.files(
    path = "corpus_sampled_opt/", full.names = T)
    )

  features <- make.frequency.list(tokenised_texts, head = 2000)

  data <- make.table.of.frequencies(tokenised_texts, 
                                  features,
                                  relative = TRUE)[,1:mfw]

  s_words <- str_detect(colnames(data), signal_words)

  data <- data[, !s_words]

  rownames(data) <- str_remove_all(rownames(data), "^.*?//")

  dt <- dist.wurzburg(data) %>% as.dist(upper = T, diag = T)
  d_long <- process_distances(dt)

  d_res_w[[i]] <- d_long
}

saveRDS(d_res_w,file="d_res_w.rds")
```

```{r}
d_res_w <- readRDS("d_res_w.rds")
```

```{r}
lannister = c("#5C0000", "#890000", "#C50000", "#FB7E00", "#FFA700")

d_df <- d_res_w %>% 
  bind_rows() %>% 
  group_by(source, target) %>% 
  mutate(mean = mean(distance)) 

head(d_df)
```

Select mean pointer

```{r}
unique(d_df$target)

x <- unique(d_df$target)[6]
y <- unique(d_df$target)[7]

selected_mean_x <- d_df %>% 
  filter(source == x & target == x) %>% 
  pull(mean) %>% unique()

selected_mean_y <- d_df %>% 
  filter(source == y & target == y) %>% 
  pull(mean) %>% unique()
```

```{r}
d_df %>% 
  plot_distances() + 
  geom_vline(data=. %>% filter(target==x), 
             aes(xintercept=selected_mean_x), 
             color="green",
             linewidth=1) + 
  geom_vline(data=. %>% filter(target==y), 
             aes(xintercept=selected_mean_y), 
             color="blue",
             linewidth=1)
```

```{r}
ggsave("dist_plot.png", plot = last_plot(), 
       bg = "white", width = 20, height = 10, dpi = 300)
```
